{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "111c4a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/media/wen/vingt/NUS_MODULES/sph6004/assignment2/for_submission', '', '/opt/ros/melodic/lib/python2.7/dist-packages', '/home/wen/bus_ws/src/spconv/spconv', '/media/wen/vingt/det_methods/Continental_Radar/second.pytorch/second', '/media/wen/vingt/det_methods/Continental_Radar/second.pytorch', '/usr/lib/python38.zip', '/usr/lib/python3.8', '/usr/lib/python3.8/lib-dynload', '/home/wen/virtualenv_py/sph/lib/python3.8/site-packages']\n",
      "1.11.0+cu102\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "print(sys.path)\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "print(torch.__version__)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tsai.all import *\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7a6be10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set sizes: (1212, 22) (4220, 23)\n",
      "Test set sizes: (515, 22) (1791, 23)\n"
     ]
    }
   ],
   "source": [
    "## Get static and dynamic training & testing dataset \n",
    "\n",
    "input_date = '0407'\n",
    "input_impute = 'knn_impute' #'with_missing' #mean_impute\n",
    "train_static_path = 'data/train set/train_static_{}.csv'.format(input_date)\n",
    "df_train_static = pd.read_csv(train_static_path)\n",
    "\n",
    "train_dyn_path = 'data/train set/train_dynamic_{}_{}.csv'.format(input_impute, input_date)\n",
    "df_train_dynamic = pd.read_csv(train_dyn_path)\n",
    "\n",
    "\n",
    "test_static_path = 'data/test set/test_static_{}.csv'.format(input_date)\n",
    "df_test_static = pd.read_csv(test_static_path)\n",
    "\n",
    "test_dyn_path = 'data/test set/test_dynamic_{}_{}.csv'.format(input_impute, input_date)\n",
    "df_test_dynamic = pd.read_csv(test_dyn_path)\n",
    "\n",
    "\n",
    "# df_train_static = df_train_static[df_train_static['vent_start_hour']<35]\n",
    "# df_test_static = df_test_static[df_test_static['vent_start_hour']<35]\n",
    "\n",
    "print('Train set sizes:', df_train_static.shape, df_train_dynamic.shape)\n",
    "print('Test set sizes:',df_test_static.shape, df_test_dynamic.shape)\n",
    "\n",
    "assert len(df_train_static[df_train_static['vent_start_hour']>60]) ==0 \n",
    "assert len(df_test_static[df_test_static['vent_start_hour']>60]) ==0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cf50629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 patients with vent start hour greater than 60\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} patients with vent start hour greater than 60\".format(len(df_test_static[df_test_static['vent_start_hour']>60])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f7537a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_train_static.plot.hist(column=[\"vent_start_hour_1\", \"vent_end_hour_1\"], bins=12, figsize=(10, 8),  range=[0, 60], alpha=0.5)\n",
    "ax.set_title('Ventilation timing')\n",
    "ax.set_xlabel('Hours')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "44bdc139",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_sequences(df_dyn_data, df_static, len_series):\n",
    "    '''\n",
    "    function takes in dynamic and static data and len series. \n",
    "    1. For each patient we parse the data for each patient with the chart time \n",
    "    2. Generate a subsequence for each patient of length : len_series\n",
    "    '''\n",
    "    count_ = np.zeros((60,1))\n",
    "    labels = np.array([]).reshape(0,2)\n",
    "    dyna_series_train =[]\n",
    "    count = 0 \n",
    "    num_df_sample = 0 \n",
    "\n",
    "    for i, x in enumerate(df_static['stay_id']):\n",
    "    #     print(x)\n",
    "        df_sample = df_dyn_data[df_dyn_data['stay_id']==x] \n",
    "\n",
    "        num_df_sample+=(len(df_sample))\n",
    "        df_label = df_static[df_static['stay_id']==x]    \n",
    "        temp_sample = df_label.loc[:,['vent_start_hour_1', 'vent_duration_1']].values\n",
    "        labels = np.vstack((labels, temp_sample)) #.values[0]  \n",
    "        #for idx_s in range(len(df_sample)):\n",
    "\n",
    "        if len(df_sample) < len_series: \n",
    "            while len(df_sample)< len_series: \n",
    "                df_dyn_data.loc[-1, 'stay_id'] = x\n",
    "                df_dyn_data = df_dyn_data.sort_values('stay_id').reset_index(drop=True)\n",
    "                df_dyn_data = df_dyn_data.interpolate()\n",
    "                df_sample = df_dyn_data[df_dyn_data['stay_id']==x] \n",
    "            df_sample = df_sample.interpolate()\n",
    "\n",
    "        zeroed_sample = df_sample[-len_series:].iloc[:, 3:].values\n",
    "        assert zeroed_sample.shape[0] ==len_series\n",
    "        dyna_series_train.append(zeroed_sample)\n",
    "\n",
    "    temp = torch.tensor(dyna_series_train)\n",
    "    train_dyn_set = temp.permute(0,2,1)\n",
    "    # labels[:,0] = (labels[:,0] - 36)/60\n",
    "    # print(count)\n",
    "    #labels_tensor = torch.from_numpy(np.round(labels))\n",
    "    return train_dyn_set, labels\n",
    "\n",
    "\n",
    "\n",
    "def normalise_dyn(df_dyn):\n",
    "    normalise_idx = 6\n",
    "    df_dyn = df_dyn.drop(['charttime_hour_2', 'charttime_hour_1' , 'charttime_hour_6' , 'charttime_hour_12' , 'charttime_hour_24'], axis=1)\n",
    "    df_dyn.iloc[:, 3:]  = (df_train_dynamic.iloc[:, 3:] - df_train_dynamic.iloc[:, 3:].mean()) / df_train_dynamic.iloc[:, 3:].std()\n",
    "\n",
    "    return df_dyn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "89148dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MIMICDataset(Dataset):\n",
    "    '''\n",
    "    function generates a torch data loader type for training\n",
    "    '''\n",
    "    def __init__(self, data_dyn , data_labels):\n",
    "        self.data = data_dyn\n",
    "        self.labels_all = data_labels\n",
    " \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        data_sampled = self.data[idx, :]\n",
    "        label_sampled =  self.labels_all[idx,:]       \n",
    "        sample = {'data': data_sampled , 'labels':label_sampled }\n",
    "        return sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01ad07a7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, len_series, a= 20, d =10):\n",
    "        super().__init__()\n",
    "        #self.conv1 = nn.Conv1d(15, 20, 2)\n",
    "        self.conv1 = nn.Conv1d(15, a, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(a)\n",
    "        self.conv2 = nn.Conv1d(a, d, 1)\n",
    "        self.pool = nn.MaxPool2d(1, 1)\n",
    "        self.fc1 = nn.Linear(len_series*d, 4)\n",
    "        # self.fc1 = nn.Linear(120, 4) #len series 5\n",
    "        # self.fc2 = nn.Linear(4, 4)\n",
    "        self.fc3 = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = self.fc1(x)\n",
    "        # x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fd265b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_models(str_input , pred):\n",
    "    eval_results = []\n",
    "    ## str_input should be : start, end, duration \n",
    "    test_static_path = 'data/test set/test_static_{}.csv'.format(input_date)\n",
    "    df_test_static = pd.read_csv(test_static_path)\n",
    "    for num_hours in [1, 2, 6, 12, 24]:\n",
    "        vent_hour_ =  'vent_{}_{}'.format(str_input, num_hours)  \n",
    "        if num_hours ==1: \n",
    "            labels =  df_test_static[vent_hour_]\n",
    "            L1_abs_err  = np.mean(abs(labels - pred))\n",
    "            print('Mean L1 error:', np.round(L1_abs_err,2))\n",
    "            eval_results.append(np.round(L1_abs_err,2))\n",
    "            MSE  = np.mean((labels -pred )**2)\n",
    "            print('Mean Square error:', np.round(MSE,2))\n",
    "            eval_results.append( np.round(MSE,2))\n",
    "        output_hours  = np.round(pred/num_hours)\n",
    "        acc_hours = np.sum(output_hours == df_test_static[vent_hour_])/ len(df_test_static)*100\n",
    "        eval_results.append(np.round(acc_hours,2))\n",
    "        print('Accuracy at {} hours : {} %'.format(num_hours, np.round(acc_hours,2)))\n",
    "    return eval_results\n",
    "\n",
    "\n",
    "def train_model (model_name, c_in, lr, drop_out, len_series, isPCA, param_name ):\n",
    "\n",
    "    print('Model training: param - ', param_name)\n",
    "    loss_count = 0 \n",
    "    loss_init = 0 \n",
    "    loss_graph = []; ve_1 = []; ve_2 = []\n",
    "    d = 200 \n",
    "    if model_name =='CNN':\n",
    "        print('Using CNN')\n",
    "        net = Net(len_series)\n",
    "    elif model_name == 'InceptionTime': \n",
    "        # print('Using Inception Time')\n",
    "        # net = InceptionTime(c_in=c_in,c_out=2, seq_len=len_series) #, nf=a, depth = d)\n",
    "        print('Using Inception Time with dropouts and batch norm')\n",
    "        net =  InceptionTimePlus (c_in=c_in, c_out=2, seq_len=len_series, fc_dropout=0.05,\n",
    "                    bn=False, ks=40, bottleneck=True, separable=False, dilation=1, stride=1,\n",
    "                    conv_dropout=drop_out, sa=False, se=None, norm='Batch')\n",
    "    elif model_name == 'TST':\n",
    "        net = TST(c_in=15, c_out=2, seq_len=len_series, d_model=128, n_heads=16,  dropout=drop_out, n_layers=3,\n",
    "            fc_dropout=0.1)\n",
    "\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    net.to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    # criterion = nn.L1Loss()\n",
    "    # criterion = nn.SmoothL1Loss()\n",
    "    # optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "   \n",
    "    # optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n",
    "    # scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0001, max_lr=0.001)\n",
    "    for epoch in range(200):  # loop over the dataset multiple times\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        for data_all in train_loader:\n",
    "            inputs = data_all['data'].to(device)\n",
    "            gt = data_all['labels'].float().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs.float())\n",
    "            # loss = criterion(outputs , gt) \n",
    "            loss = criterion(outputs[:,0] , gt[:,0] ) +   criterion(outputs[:,1], gt[:,1])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        if abs(loss_init- running_loss/len(train_set) ) < 0.01:\n",
    "            loss_count +=1\n",
    "        if loss_count > 5:\n",
    "            break\n",
    "        loss_init = running_loss/len(train_set)\n",
    "        loss_graph.append(running_loss/len(train_set))\n",
    "        \n",
    "        net.eval() \n",
    "        outputs = net(dyn_test.float().to(device))\n",
    "        label_all = torch.from_numpy(np.array(labels_test)).float().to(device)\n",
    "\n",
    "        val_error = torch.mean(abs(torch.round(outputs[:,0]) - label_all[:,0]))\n",
    "        ve_1.append(val_error.cpu().detach().numpy().tolist())\n",
    "        val_error2 = torch.mean(abs(torch.round(outputs[:,0]) - label_all[:,0]))\n",
    "        ve_2.append(val_error2.cpu().detach().numpy().tolist())\n",
    "        if epoch % 20 == 0:\n",
    "            # print(f'[{epoch + 1}] loss: {running_loss/len(train_set):.3f}')\n",
    "            val_error = torch.mean(abs(torch.round(outputs[:,0]) - label_all[:,0]))\n",
    "            # print('Mean L1 error:', val_error)\n",
    "\n",
    "\n",
    "    outputs = net(dyn_test.float().to(device))\n",
    "    outputs = outputs.cpu().detach().numpy()\n",
    "    temp = []\n",
    "    print('===================  VENTILATION START TIME  ============================= ')\n",
    "    eval_results = evaluate_models('start_hour', outputs[:,0] )\n",
    "    if param_name == 'lr':  \n",
    "        temp.extend([lr])\n",
    "    elif param_name == 'drop_out':\n",
    "        temp.extend([drop_out])\n",
    "    elif param_name == 'len_series':\n",
    "        temp.extend([len_series])\n",
    "    \n",
    "    temp.extend(eval_results)\n",
    "    print('===================  VENTILATION DURATION TIME  ============================= ')\n",
    "    eval_results2 = evaluate_models('duration', outputs[:,1] )\n",
    "    temp.extend(eval_results2)\n",
    "    total = round(eval_results[6] + eval_results2[6], 2)\n",
    "    if (total) > 50:\n",
    "        print('Saving model..')\n",
    "        torch.save(net.state_dict(), 'best_model_inception_time_{}_withPCA_{}.pt'.format(total,isPCA))\n",
    "    else: \n",
    "        # torch.save(net, 'curr_model_inception_time_{}_withPCA_{}.pt'.format(eval_results2[3]))\n",
    "        print('No model saved - current model, performance : {}'.format(total) )\n",
    "    print('Finished Training')\n",
    "    return temp, outputs, loss_graph, ve_1, ve_2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92f9241f",
   "metadata": {},
   "source": [
    "## Grid search function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dc5fa0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: drop_out\n",
      "torch.Size([1212, 15, 4]) (1212, 2)\n",
      "torch.Size([515, 15, 4]) (515, 2)\n",
      "torch.Size([1212, 15, 4]) (1212, 2)\n",
      "torch.Size([515, 15, 4]) (515, 2)\n",
      "C IN 15\n",
      "Training: drop_out\n",
      "torch.Size([1212, 15, 4]) (1212, 2)\n",
      "torch.Size([515, 15, 4]) (515, 2)\n",
      "torch.Size([1212, 15, 4]) (1212, 2)\n",
      "torch.Size([515, 15, 4]) (515, 2)\n",
      "C IN 15\n",
      "Training: drop_out\n",
      "torch.Size([1212, 15, 4]) (1212, 2)\n",
      "torch.Size([515, 15, 4]) (515, 2)\n",
      "torch.Size([1212, 15, 4]) (1212, 2)\n",
      "torch.Size([515, 15, 4]) (515, 2)\n",
      "C IN 15\n",
      "Training: drop_out\n",
      "torch.Size([1212, 15, 4]) (1212, 2)\n",
      "torch.Size([515, 15, 4]) (515, 2)\n",
      "torch.Size([1212, 15, 4]) (1212, 2)\n",
      "torch.Size([515, 15, 4]) (515, 2)\n",
      "C IN 15\n",
      "Training: drop_out\n",
      "torch.Size([1212, 15, 4]) (1212, 2)\n",
      "torch.Size([515, 15, 4]) (515, 2)\n",
      "torch.Size([1212, 15, 4]) (1212, 2)\n",
      "torch.Size([515, 15, 4]) (515, 2)\n",
      "C IN 15\n"
     ]
    }
   ],
   "source": [
    "\n",
    "info = []\n",
    "lr = 0.001\n",
    "len_series = 4\n",
    "model_name = 'InceptionTime'\n",
    "drop_out = 0.05 \n",
    "isPCA = False \n",
    "\n",
    "loss_graph_arr =[]\n",
    "ve_1_arr= [] \n",
    "ve_2_arr =[]\n",
    "\n",
    "param_name = 'drop_out'\n",
    "params = {'lr': [0.1, 0.01, 0.001],  'len_series' :  [1, 2, 3, 4, 5, 10 , 15],  'drop_out' : [0, 0.05, 0.1, 0.15, 0.2], 'pca':[0, 1]}\n",
    "for lr  in params[param_name]:\n",
    "    print('Training:', param_name)\n",
    "\n",
    "    df_train_dynamic_norm = normalise_dyn(df_train_dynamic)\n",
    "    df_test_dynamic_norm = normalise_dyn(df_test_dynamic)\n",
    "\n",
    "    dyn_train, labels_train = get_sequences(df_train_dynamic_norm, df_train_static, len_series)\n",
    "    print(dyn_train.shape, labels_train.shape)\n",
    "    dyn_test, labels_test = get_sequences(df_test_dynamic_norm, df_test_static, len_series)\n",
    "    print(dyn_test.shape, labels_test.shape)\n",
    "    c_in = 15\n",
    "    \n",
    "    batch_size =100\n",
    "    train_set = MIMICDataset(dyn_train, labels_train)\n",
    "    test_set = MIMICDataset(dyn_test, labels_test)\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "    train_size = len(train_set)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "    test_size = len(test_set)\n",
    "    data_loaders = {\"train\": train_loader, \"test\": test_loader}\n",
    "    dataset_sizes = {\"train\": train_size, \"test\": test_size}\n",
    "    print(dyn_train.shape, labels_train.shape)\n",
    "    print(dyn_test.shape, labels_test.shape)\n",
    "    print('C IN', c_in)\n",
    "\n",
    "    ## Train the model, get the outputs, loss graph and errors for start of ventilation (ve_1) and duration (ve_2)\n",
    "    temp, outputs, loss_graph, ve_1, ve_2 = train_model (model_name= model_name, c_in = c_in, lr=lr, drop_out = drop_out, len_series = len_series , isPCA= isPCA, param_name = param_name)\n",
    "    loss_graph_arr.append(loss_graph)\n",
    "    ve_1_arr.append(ve_1) \n",
    "    ve_2_arr.append(ve_2)\n",
    "    info.append(temp)\n",
    "    df_tuning = pd.DataFrame(info, columns=['drop_out', 'L1', 'MSE', 'start_1', 'start_2', 'start_6',  'start_12', 'start_24', 'vent_L1' , 'vent_MSE' , 'vent_1'  , 'vent_2'  , 'vent_6'  , 'vent_12'  , 'vent_24' ])\n",
    "    df_tuning.to_csv('tuning_0414_it_{}_nopca.csv'.format(param_name))  \n",
    "    # return outputs, loss_graph, ve_1, ve_2 \n",
    "\n",
    "print('Finished searching..')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c28d0e3",
   "metadata": {},
   "source": [
    "## Script for visualising loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e792dea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,figsize = (15,5))\n",
    "label =  params[param_name]\n",
    "title_ = ['Loss', 'Start L1 Err', 'Duration L1 err']\n",
    "y_label_ = ['Loss', 'Error', 'Error']\n",
    "for idx in range(0, 3):\n",
    "    num_epochs  = range(len(loss_graph))\n",
    "    if idx == 0: \n",
    "        for enum_l, loss_graph in enumerate(loss_graph_arr): \n",
    "            ax[idx].plot(num_epochs, loss_graph, label = param_name + '= '+ str(label[enum_l])) \n",
    "    elif idx == 1: \n",
    "        for enum_l, ve_1 in enumerate(ve_1_arr):       \n",
    "            ax[idx].plot(num_epochs, ve_1, label = param_name +'= '+ str(label[enum_l])) \n",
    "        ax[idx].set_xlim(0,150)\n",
    "        ax[idx].set_ylim(0,150)\n",
    "    elif idx == 2: \n",
    "        ax[idx].set_xlim(0,150)\n",
    "        for enum_l, ve_2 in enumerate(ve_2_arr):  \n",
    "            ax[idx].plot(num_epochs, ve_2, label = param_name+ ' = '+ str(label[enum_l]))\n",
    "    ax[idx].set_xlabel('Epoch')\n",
    "    ax[idx].set_ylabel('{}'.format(y_label_[idx]))\n",
    "\n",
    "    ax[idx].title.set_text('{}'.format(title_[idx]))\n",
    "    ax[idx].legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3eb0b103",
   "metadata": {},
   "source": [
    "## Visualise the loss and test set error graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e0ff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig_plots(loss_graph, ve_1, ve_2):\n",
    "    fig,ax = plt.subplots(1,3,figsize = (15,5))\n",
    "    title_ = ['Loss', 'Start L1 Err', 'Duration L1 err']\n",
    "    y_label_ = ['Loss', 'Error', 'Error']\n",
    "    for idx in range(0, 3):\n",
    "        num_epochs  = range(len(loss_graph))\n",
    "        if idx == 0: \n",
    "            ax[idx].plot(num_epochs, loss_graph) \n",
    "        elif idx == 1: \n",
    "            ax[idx].plot(num_epochs, ve_1) \n",
    "        elif idx == 2: \n",
    "            ax[idx].plot(num_epochs, ve_2)\n",
    "        ax[idx].set_xlabel('Epoch')\n",
    "        ax[idx].set_ylabel('{}'.format(y_label_[idx]))\n",
    "\n",
    "        ax[idx].title.set_text('{}'.format(title_[idx]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccfa2c77",
   "metadata": {},
   "source": [
    "## Evaluation Script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a8d253",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('===================  VENTILATION START TIME  ============================= ')\n",
    "evaluate_models('start_hour', outputs[:,0] )\n",
    "\n",
    "print('===================  VENTILATION DURATION TIME  ============================= ')\n",
    "evaluate_models('duration', outputs[:,1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497ff9f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
